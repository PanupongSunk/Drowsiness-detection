{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q_rdsgvibTC"
      },
      "outputs": [],
      "source": [
        "#install library from other\n",
        "!pip install mediapipe\n",
        "!pip install neurokit2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.signal import butter, sosfiltfilt, find_peaks, welch\n",
        "from statistics import multimode,mean\n",
        "from neurokit2.signal import signal_smooth\n",
        "from scipy.stats import entropy\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.integrate import trapz"
      ],
      "metadata": {
        "id": "6SP7gU-FiwzE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and un-zip ex videos\n",
        "! gdown 1rsaa8Q0vQmkmzlLLhMkFo-qpLWNM-GC5\n",
        "!unzip /content/Example_video.zip\n",
        "# Download and un-zip model\n",
        "! gdown 1imOtgGaV88ij8LcC3eJMOPQGtjFLgzTf\n",
        "!unzip /content/NN_model.zip"
      ],
      "metadata": {
        "id": "ySdOBsRcP1Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import tools from mediapipe\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "#select the (face) model type from mediapipe\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)"
      ],
      "metadata": {
        "id": "MYYl3CCUi0mu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define forehead area by selecting the interest points on subject's face\n",
        "def forehead_define():\n",
        "    forehead_1 = np.array((landmark_points[103], landmark_points[67], landmark_points[104]))\n",
        "    forehead_2 = np.array((landmark_points[67], landmark_points[104], landmark_points[69]))\n",
        "    forehead_3 = np.array((landmark_points[108], landmark_points[69], landmark_points[67]))\n",
        "    forehead_4 = np.array((landmark_points[109], landmark_points[108], landmark_points[67]))\n",
        "    forehead_5 = np.array((landmark_points[109], landmark_points[151], landmark_points[108]))\n",
        "    forehead_6 = np.array((landmark_points[10], landmark_points[109], landmark_points[151]))\n",
        "    forehead_7 = np.array((landmark_points[10], landmark_points[151], landmark_points[338]))\n",
        "    forehead_8 = np.array((landmark_points[151], landmark_points[337], landmark_points[338]))\n",
        "    forehead_9 = np.array((landmark_points[338], landmark_points[337], landmark_points[297]))\n",
        "    forehead_10 = np.array((landmark_points[337], landmark_points[299], landmark_points[297]))\n",
        "    forehead_11 = np.array((landmark_points[297], landmark_points[299], landmark_points[333]))\n",
        "    forehead_12 = np.array((landmark_points[297], landmark_points[333], landmark_points[332]))\n",
        "    forehead_13 = np.array((landmark_points[104], landmark_points[69], landmark_points[105]))\n",
        "    forehead_14 = np.array((landmark_points[105], landmark_points[66], landmark_points[69]))\n",
        "    forehead_15 = np.array((landmark_points[69], landmark_points[66], landmark_points[107]))\n",
        "    forehead_16 = np.array((landmark_points[108], landmark_points[69], landmark_points[107]))\n",
        "    forehead_17 = np.array((landmark_points[108], landmark_points[107], landmark_points[9]))\n",
        "    forehead_18 = np.array((landmark_points[151], landmark_points[108], landmark_points[9]))\n",
        "    forehead_19 = np.array((landmark_points[151], landmark_points[9], landmark_points[337]))\n",
        "    forehead_20 = np.array((landmark_points[9], landmark_points[336], landmark_points[337]))\n",
        "    forehead_21 = np.array((landmark_points[337], landmark_points[336], landmark_points[299]))\n",
        "    forehead_22 = np.array((landmark_points[336], landmark_points[296], landmark_points[299]))\n",
        "    forehead_23 = np.array((landmark_points[299], landmark_points[296], landmark_points[334]))\n",
        "    forehead_24 = np.array((landmark_points[299], landmark_points[334], landmark_points[333]))\n",
        "    return forehead_1,forehead_2,forehead_3,forehead_4,forehead_5,forehead_6,forehead_7,forehead_8,forehead_9,forehead_10,forehead_11,forehead_12,forehead_13,forehead_14,forehead_15,forehead_16,forehead_17,forehead_18,forehead_19,forehead_20,forehead_21,forehead_22,forehead_23,forehead_24\n",
        "\n",
        "# define mask\n",
        "def mask_define(h,w):\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    return mask\n",
        "\n",
        "# define BP filter\n",
        "def bandpass_filter(data, lowcut, highcut,fs):\n",
        "    nyq = 0.5 * fs\n",
        "    low = float(lowcut) / float(nyq)\n",
        "    high = float(highcut) / float(nyq)\n",
        "    order = 3.0\n",
        "    sos = butter(order, [low, high], btype='band',output='sos')\n",
        "    #Change filt or filtfilt here\n",
        "#     bandpass = sosfilt(sos, data)\n",
        "    bandpass = sosfiltfilt(sos, data)\n",
        "    return bandpass\n",
        "\n",
        "# define modified POS\n",
        "def POS(red, green, blue, frame, low, high,fs):\n",
        "    win_size = 30\n",
        "    H = np.zeros(frame)\n",
        "    idx = 0\n",
        "    while True:\n",
        "        R_interval_norm = red[idx:idx+win_size]/red[idx:idx+win_size].mean()\n",
        "        G_interval_norm = green[idx:idx+win_size]/green[idx:idx+win_size].mean()\n",
        "        B_interval_norm = blue[idx:idx+win_size]/blue[idx:idx+win_size].mean()\n",
        "        color_arr = np.array((R_interval_norm, G_interval_norm, B_interval_norm))\n",
        "        S_1 = (-0.168*R_interval_norm) - (0.331*G_interval_norm) + (0.499*B_interval_norm)\n",
        "        S_2 = (0.499*R_interval_norm) -  (0.418*G_interval_norm) - (0.081*B_interval_norm)\n",
        "        alpha = S_1.std()/S_2.std()\n",
        "        h = S_1 + (alpha * S_2)\n",
        "        H[idx:idx+win_size] = H[idx:idx+win_size] + (h - h.mean())\n",
        "        if idx+win_size > frame:\n",
        "            break\n",
        "        idx = idx + 1\n",
        "    return bandpass_filter(H,low,high,fs)\n",
        "\n",
        "# Extract the number of sample points in each area from the whole data\n",
        "def extract_data(red_arr,green_arr,blue_arr,idx,sample_point_num,area_num):\n",
        "    tmp_signal_red,tmp_signal_green, tmp_signal_blue= [],[],[]\n",
        "    red_color_arr, green_color_arr, blue_color_arr = [],[],[]\n",
        "    for i in range(area_num):\n",
        "        tmp_signal_red.append(red_arr[i][idx:idx+sample_point_num])\n",
        "        tmp_signal_green.append(green_arr[i][idx:idx+sample_point_num])\n",
        "        tmp_signal_blue.append(blue_arr[i][idx:idx+sample_point_num])\n",
        "\n",
        "        red_now = np.array(tmp_signal_red.copy())\n",
        "        green_now = np.array(tmp_signal_green.copy())\n",
        "        blue_now = np.array(tmp_signal_blue.copy())\n",
        "\n",
        "        red_color_arr.append(red_now)\n",
        "        green_color_arr.append(green_now)\n",
        "        blue_color_arr.append(blue_now)\n",
        "\n",
        "        tmp_signal_red.clear()\n",
        "        tmp_signal_green.clear()\n",
        "        tmp_signal_blue.clear()\n",
        "    return red_color_arr,green_color_arr,blue_color_arr\n",
        "\n",
        "# reshape data\n",
        "def reshape_data(red,green,blue,area_num,sample_point_num):\n",
        "    red_color_arr_np = np.array(red.copy())\n",
        "    green_color_arr_np = np.array(green.copy())\n",
        "    blue_color_arr_np = np.array(blue.copy())\n",
        "    red_color_arr_np = np.reshape( red_color_arr_np,(area_num,sample_point_num))\n",
        "    green_color_arr_np = np.reshape( green_color_arr_np,(area_num,sample_point_num))\n",
        "    blue_color_arr_np = np.reshape( blue_color_arr_np,(area_num,sample_point_num))\n",
        "    return red_color_arr_np,green_color_arr_np,blue_color_arr_np\n",
        "\n",
        "#Find hair area\n",
        "def find_hair(red_arr,green_arr,blue_arr):\n",
        "    red_no_hair,green_no_hair , blue_no_hair = [],[],[]\n",
        "    max_green = np.max(green_arr.mean(axis=1))\n",
        "\n",
        "    #Find the average value of green color in each area\n",
        "    tmp_green = green_arr.mean(axis=1)\n",
        "\n",
        "    #Find which area is the hair\n",
        "    hair_area_green = np.where(tmp_green/max_green<0.7)\n",
        "\n",
        "    # Find which index is the hair then remove it\n",
        "    for i in range(len(green_arr)):\n",
        "        if (i == hair_area_green[0]).any():\n",
        "            continue\n",
        "        else:\n",
        "            red_no_hair.append(red_arr[i])\n",
        "            green_no_hair.append(green_arr[i])\n",
        "            blue_no_hair.append(blue_arr[i])\n",
        "    red_no_hair_arr = np.array(red_no_hair)\n",
        "    green_no_hair_arr = np.array(green_no_hair)\n",
        "    blue_no_hair_arr = np.array(blue_no_hair)\n",
        "    return red_no_hair_arr,green_no_hair_arr,blue_no_hair_arr\n",
        "\n",
        "#Find rppg\n",
        "def find_rppg(red,green,blue,low,high,fs,area_num,sample_point_num):\n",
        "    signal = []\n",
        "    for i in range(area_num):\n",
        "        S_pos = POS(red[i], green[i], blue[i],sample_point_num,low,high,fs)\n",
        "        signal.append(S_pos)\n",
        "        signal_arr = np.array(signal.copy())\n",
        "    # Find average value of the rPPG\n",
        "    S_pos_avg_normal = signal_arr.mean(axis=0)\n",
        "    return S_pos_avg_normal\n",
        "\n",
        "#Find hr\n",
        "def find_hr(S_pos,fft_point,df):\n",
        "    s_f = 20*np.log10(np.abs(np.fft.fft(S_pos,n=fft_point) + pow(10,-10)))\n",
        "    hr = np.argmax(s_f[:fft_point//2]) * df * 60\n",
        "    return hr\n",
        "\n",
        "#Find group hr (Majority vote)\n",
        "def find_group(hr):\n",
        "    num_div = 5\n",
        "    while True:\n",
        "        hr_tmp_new = (np.array(hr)//num_div) * num_div\n",
        "        mode_hr = multimode(hr_tmp_new)\n",
        "        mode_hr_arr = np.array(mode_hr)\n",
        "        if len(mode_hr_arr) == 1:\n",
        "            mode_val = mode_hr_arr\n",
        "            break\n",
        "        elif len(mode_hr_arr) > 2:\n",
        "            num_div += 5\n",
        "        else:\n",
        "            if np.abs(np.diff(mode_hr_arr)) == num_div:\n",
        "                mode_val = mode_hr_arr.mean()\n",
        "                break\n",
        "            else:\n",
        "                num_div += 5\n",
        "    return mode_val,num_div\n",
        "\n",
        "# Peak detection from neurokit 2\n",
        "def ppg_findpeaks_elgendi(\n",
        "    signal,\n",
        "    sampling_rate,\n",
        "    peakwindow=0.111, #0.111\n",
        "    beatwindow=0.667, #0.667\n",
        "    beatoffset=0.02, #0.02\n",
        "    mindelay=0.25, #0.3\n",
        "    show=False,\n",
        "):\n",
        "    # Ignore the samples with negative amplitudes and square the samples with\n",
        "    # values larger than zero.\n",
        "    signal_abs = signal.copy()\n",
        "    signal_abs[signal_abs < 0] = 0\n",
        "    sqrd = signal_abs ** 2\n",
        "\n",
        "    # Compute the thresholds for peak detection. Call with show=True in order\n",
        "    # to visualize thresholds.\n",
        "    ma_peak_kernel = int(np.rint(peakwindow * sampling_rate))\n",
        "    ma_peak = signal_smooth(sqrd, kernel=\"boxcar\", size=ma_peak_kernel)\n",
        "\n",
        "    ma_beat_kernel = int(np.rint(beatwindow * sampling_rate))\n",
        "    ma_beat = signal_smooth(sqrd, kernel=\"boxcar\", size=ma_beat_kernel)\n",
        "\n",
        "    thr1 = ma_beat + beatoffset * np.mean(sqrd)  # threshold 1\n",
        "\n",
        "    # Identify start and end of PPG waves.\n",
        "    waves = ma_peak > thr1\n",
        "    beg_waves = np.where(np.logical_and(np.logical_not(waves[0:-1]), waves[1:]))[0]\n",
        "    end_waves = np.where(np.logical_and(waves[0:-1], np.logical_not(waves[1:])))[0]\n",
        "    # Throw out wave-ends that precede first wave-start.\n",
        "    end_waves = end_waves[end_waves > beg_waves[0]]\n",
        "\n",
        "    # Identify systolic peaks within waves (ignore waves that are too short).\n",
        "    num_waves = min(beg_waves.size, end_waves.size)\n",
        "    min_len = int(\n",
        "        np.rint(peakwindow * sampling_rate)\n",
        "    )  # this is threshold 2 in the paper\n",
        "    min_delay = int(np.rint(mindelay * sampling_rate))\n",
        "    peaks = []\n",
        "\n",
        "    for i in range(num_waves):\n",
        "\n",
        "        beg = beg_waves[i]\n",
        "        end = end_waves[i]\n",
        "        len_wave = end - beg\n",
        "\n",
        "        if len_wave < min_len:\n",
        "            continue\n",
        "\n",
        "        # Find local maxima and their prominence within wave span.\n",
        "        data = signal[beg:end]\n",
        "        locmax, props = find_peaks(data, prominence=(None, None))\n",
        "\n",
        "        if locmax.size > 0:\n",
        "            # Identify most prominent local maximum.\n",
        "            peak = beg + locmax[np.argmax(props[\"prominences\"])]\n",
        "#             print(peaks,i)\n",
        "            # Enforce minimum delay between peaks.\n",
        "            if i == 0 or len(peaks) == 0:\n",
        "                peaks.append(peak)\n",
        "            else:\n",
        "                if peak - peaks[-1] > min_delay:\n",
        "                    peaks.append(peak)\n",
        "\n",
        "    peaks = np.asarray(peaks).astype(int)\n",
        "    return peaks\n",
        "\n",
        "def prv(peak,fs):\n",
        "    #convert peak idx to milisecond\n",
        "    rri = (np.diff(peak)/fs) *1000\n",
        "    dif_rri = np.diff(rri)\n",
        "    mean_pp = rri.mean()\n",
        "    rmssd = np.sqrt(pow(dif_rri,2).mean())\n",
        "    sdnn = np.std(rri,ddof=1)\n",
        "    sdsd = np.std(dif_rri,ddof=1)\n",
        "    return mean_pp,rmssd,sdnn,sdsd\n",
        "\n",
        "def prv_nonlinear(peak,fs):\n",
        "    #sd1,sd2\n",
        "    rri = (np.diff(peak)/fs) *1000\n",
        "    rri_n = rri[:-1]\n",
        "    rri_p = rri[1:]\n",
        "    x1 = (rri_n - rri_p) / np.sqrt(2)\n",
        "    x2 = (rri_n + rri_p) / np.sqrt(2)\n",
        "    sd1 = np.std(x1)\n",
        "    sd2 = np.std(x2)\n",
        "    #ShanEn\n",
        "    freq = np.unique(rri)\n",
        "    Shan = entropy(freq,base=2)\n",
        "    return sd1,sd2,Shan\n",
        "\n",
        "def interpolate(peak,fs):\n",
        "    rri = (np.diff(peak)/fs) *1000\n",
        "    steps = 1/fs\n",
        "    x = np.cumsum(rri) / 1000.0\n",
        "    f = interp1d(x, rri, fill_value=\"extrapolate\", kind='cubic')\n",
        "    xx = np.arange(1, np.max(x), steps)\n",
        "    rri_interpolated = f(xx)\n",
        "    return rri_interpolated\n",
        "\n",
        "def frequency_domain(rri, fs=30):\n",
        "    # Estimate the spectral density using Welch's method\n",
        "    fxx, pxx = welch(x=rri, fs=fs, nperseg=len(rri), nfft=3600)\n",
        "\n",
        "    '''\n",
        "    Segement found frequencies in the bands\n",
        "     - Very Low Frequency (VLF): 0-0.04Hz\n",
        "     - Low Frequency (LF): 0.04-0.15Hz\n",
        "     - High Frequency (HF): 0.15-0.4Hz\n",
        "    '''\n",
        "#     cond_vlf = (fxx >= 0) & (fxx < 0.04)\n",
        "    cond_lf = (fxx >= 0.04) & (fxx < 0.15)\n",
        "    cond_hf = (fxx >= 0.15) & (fxx < 0.4)\n",
        "\n",
        "    # calculate power in each band by integrating the spectral density\n",
        "#     vlf = trapz(pxx[cond_vlf], fxx[cond_vlf])\n",
        "    lf = trapz(pxx[cond_lf], fxx[cond_lf])\n",
        "    hf = trapz(pxx[cond_hf], fxx[cond_hf])\n",
        "\n",
        "    # sum these up to get total power\n",
        "#     total_power = vlf + lf + hf\n",
        "    return hf,lf, lf/hf\n",
        "\n",
        "def add_data_s(data,index):\n",
        "    time = 10\n",
        "    tmp = data[index:time+index]\n",
        "    return mean(tmp)\n",
        "\n",
        "def check_predict(y):\n",
        "    if y < 0.5:\n",
        "        predict_re = \"Alert\"\n",
        "    else:\n",
        "        predict_re = \"Drowsy\"\n",
        "    return predict_re"
      ],
      "metadata": {
        "id": "zs7z0Gxai1xV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "ULycFwoCUU2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model data\n",
        "from tensorflow.keras.models import load_model\n",
        "from joblib import load\n",
        "\n",
        "NN_model = load_model('/content/model_11fea_90%.h5')\n",
        "# import the mean and std of training data for scaling the data\n",
        "sc = load('/content/sc_90%.bin')\n",
        "NN_model.summary()"
      ],
      "metadata": {
        "id": "uzjv7-BuUeaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main**"
      ],
      "metadata": {
        "id": "BPFonjQcbW-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interval_time = 20 #length of each interval used to extract rPPG from the video\n",
        "subinterval_time = 6 #length of each sub interval used to extract hr from 20s rPPG to do majority vote\n",
        "frame_time = 30*interval_time #number of video frame used to extract rPPG (FPS x time)\n",
        "fft_point = 1800\n",
        "df = 30/fft_point\n",
        "\n",
        "#set up your video path\n",
        "data_path = \"/content/alert.avi\"\n",
        "# data_path = \"/content/drowsy.avi\"\n",
        "\n",
        "# open video\n",
        "cap = cv2.VideoCapture(data_path)\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "#list use for containing variable\n",
        "red_f, green_f, blue_f= [],[],[]\n",
        "red_var, green_var, blue_var = [],[],[]\n",
        "idx, data_idx = 0,0\n",
        "count_hr,count_mpp = 0,0\n",
        "\n",
        "# HR\n",
        "tmp_hr, hr_lst, tmp_mpp = [],[],[]\n",
        "# rPPG before and after majority mode\n",
        "S_pos_tmp, S_pos_before= [],[]\n",
        "#prv\n",
        "meanPP,RMSSD,sdNN,SDSD = [],[],[],[]\n",
        "sd1_lst,sd2_lst,sd1_sd2,shanen = [],[],[],[]\n",
        "hf,lf, lhf = [],[],[]\n",
        "# counting predict result\n",
        "count_a,count_d = 0,0\n",
        "\n",
        "# MediaPipe face mesh model\n",
        "with mp_face_mesh.FaceMesh(\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5) as face_mesh: #setup face detection parameter\n",
        "\n",
        "    while cap.isOpened():\n",
        "        success, image = cap.read()\n",
        "        if success == False:\n",
        "            break\n",
        "        height, width, _ = image.shape\n",
        "        if success:\n",
        "        # To improve performance, optionally mark the image as not writeable to\n",
        "        # pass by reference.\n",
        "            image.flags.writeable = False\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            results = face_mesh.process(image)\n",
        "\n",
        "        # Draw the face mesh annotations on the image.\n",
        "            image.flags.writeable = True\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            if results.multi_face_landmarks:\n",
        "                for face_landmarks in results.multi_face_landmarks:\n",
        "\n",
        "                # convert normalization coordinate into normal coordinate\n",
        "                    landmark_points = []\n",
        "                    for i in range(0, 468):\n",
        "                        x = int(face_landmarks.landmark[i].x * width)\n",
        "                        y = int(face_landmarks.landmark[i].y * height)\n",
        "                        p = [x, y]\n",
        "                        landmark_points.append([x, y])\n",
        "\n",
        "                    # create small area in forehead\n",
        "                    forehead_1,forehead_2,forehead_3,forehead_4,forehead_5,forehead_6,forehead_7,forehead_8,forehead_9,forehead_10,forehead_11,forehead_12,forehead_13,forehead_14,forehead_15,forehead_16,forehead_17,forehead_18,forehead_19,forehead_20,forehead_21,forehead_22,forehead_23,forehead_24 = forehead_define()\n",
        "\n",
        "                    # Group each small forehead array together\n",
        "                    forehead = [forehead_1, forehead_2, forehead_3, forehead_4, forehead_5, forehead_6, forehead_7, forehead_8,forehead_9,\n",
        "                               forehead_10, forehead_11, forehead_12, forehead_13, forehead_14, forehead_15, forehead_16, forehead_17, forehead_18,\n",
        "                               forehead_19, forehead_20, forehead_21, forehead_22, forehead_23, forehead_24]\n",
        "\n",
        "                    # create small triangle line in each area\n",
        "                    for i in range(len(forehead)):\n",
        "                        cv2.polylines(image, [forehead[i]], True, (0, 255, 255), 1)\n",
        "\n",
        "                    #create mask for each area\n",
        "                    mask_forehead = []\n",
        "                    for i in range(24):\n",
        "                        mask = mask_define(height, width)\n",
        "                        mask_forehead.append(mask)\n",
        "\n",
        "                    # Fill roi of each area in each mask\n",
        "                    for i in range(len(mask_forehead)):\n",
        "                        cv2.fillPoly(mask_forehead[i], [forehead[i]], (255))\n",
        "\n",
        "                    # collect data from each of forehead area (24 values)\n",
        "                    for i in range(len(mask_forehead)):\n",
        "                        crop_img = cv2.bitwise_and(image, image, mask=mask_forehead[i])\n",
        "                        indices_list = np.where(np.any(crop_img != [0, 0, 0], axis=-1))\n",
        "                        roi_pixel_img = crop_img[indices_list]\n",
        "                        var = (roi_pixel_img == [0, 255, 255]).all(axis=-1)\n",
        "                        roi_pixel_img = roi_pixel_img[~var]\n",
        "                        blue_var.append(roi_pixel_img[:, 0].mean())\n",
        "                        green_var.append(roi_pixel_img[:, 1].mean())\n",
        "                        red_var.append(roi_pixel_img[:, 2].mean())\n",
        "                    red_f.append(red_var.copy())\n",
        "                    green_f.append(green_var.copy())\n",
        "                    blue_f.append(blue_var.copy())\n",
        "                    red_var.clear()\n",
        "                    green_var.clear()\n",
        "                    blue_var.clear()\n",
        "\n",
        "                    # create array with dimention (area x num_frame)\n",
        "                    red_arr = np.array(red_f.copy()).T\n",
        "                    green_arr = np.array(green_f.copy()).T\n",
        "                    blue_arr = np.array(blue_f.copy()).T\n",
        "\n",
        "                    area_num,frame_num = red_arr.shape\n",
        "\n",
        "                     # check if collected data >= 20s and each updated data should be 1s (30 frames)\n",
        "                    if frame_num >= frame_time and frame_num%30 == 0:\n",
        "                        red, green, blue = red_arr,green_arr,blue_arr\n",
        "\n",
        "                        # Extract color data in each area from the whole data (need only 600 samples)\n",
        "                        red_color_list,green_color_list,blue_color_list = extract_data(red,green,blue,\n",
        "                                                                                       idx,frame_time,area_num)\n",
        "                        # reshape it from (24,1,600) to (24,600)\n",
        "                        red_color_arr_np,green_color_arr_np,blue_color_arr_np = reshape_data(red_color_list,green_color_list,blue_color_list,\n",
        "                                                                                         area_num,frame_time)\n",
        "                        #Find hair and skin area, return skin area\n",
        "                        red_no_hair_arr,green_no_hair_arr,blue_no_hair_arr = find_hair(red_color_arr_np,green_color_arr_np,blue_color_arr_np)\n",
        "\n",
        "                        #Find rPPG of each skin area using Modified POS\n",
        "                        new_area_num,_ = red_no_hair_arr.shape\n",
        "                        S_pos_avg_normal = find_rppg(red_no_hair_arr,green_no_hair_arr,blue_no_hair_arr,0.5,4,30,new_area_num,frame_time)\n",
        "                        S_pos_before.append(S_pos_avg_normal)\n",
        "\n",
        "                        # Majority vote (for narrowing the frequency band of the bandpass filter)\n",
        "                        #Find hr (6s hr and move 15 frames (0.5s))\n",
        "                        frame_now = int(30*subinterval_time)\n",
        "                        idx_tmp = 0\n",
        "                        hr_tmp = []\n",
        "                        while idx_tmp <= frame_time:\n",
        "                            if idx_tmp + frame_now <= frame_time:\n",
        "                                hr_check_tmp = find_hr(S_pos_avg_normal[idx_tmp:idx_tmp+frame_now],fft_point,df)\n",
        "                                hr_tmp.append(hr_check_tmp)\n",
        "                            elif idx_tmp + frame_now > frame_time:\n",
        "                                break\n",
        "                            idx_tmp = idx_tmp + 15\n",
        "\n",
        "                        #Find mode hr\n",
        "                        mode_val,num_div = find_group(hr_tmp)\n",
        "\n",
        "                        #Find new passband frequency\n",
        "                        low_band = ((mode_val+(num_div/2))*0.7)/60\n",
        "                        high_band = ((mode_val+(num_div/2))*1.3)/60\n",
        "\n",
        "                        # bandpass old rPPG signal with the new frequency band\n",
        "                        S_pos_avg_after = bandpass_filter(S_pos_avg_normal,low_band,high_band,30)\n",
        "                        S_pos_tmp.append(S_pos_avg_after)\n",
        "\n",
        "                        # Find final hr from the improve rPPG\n",
        "                        hr_check = find_hr(S_pos_avg_after,fft_point,df)\n",
        "\n",
        "                        #Need hr with error in +- 10% (optional)\n",
        "                        if count_hr > 0:\n",
        "                            if hr_check <= hr_lst[-1]*0.9 or hr_check >= hr_lst[-1]*1.1:\n",
        "                                hr_lst.append(hr_lst[-1])\n",
        "                            else:\n",
        "                                hr_lst.append(hr_check)\n",
        "                        else:\n",
        "                            hr_lst.append(hr_check)\n",
        "                        count_hr = 1\n",
        "\n",
        "                        #Find positive peak for each 20 second intervals of improve rPPG signal\n",
        "                        p = ppg_findpeaks_elgendi(S_pos_avg_after,30)\n",
        "\n",
        "                        #Find time domain prv\n",
        "                        mean_pp,rmssd,sdnn,sdsd = prv(p,30)\n",
        "\n",
        "                        #Need meanPP with error in +- 10% (optional)\n",
        "                        if count_mpp > 0:\n",
        "                            if mean_pp <= meanPP[-1]*0.9 or mean_pp >= meanPP[-1]*1.1:\n",
        "                                meanPP.append(meanPP[-1])\n",
        "                            else:\n",
        "                                meanPP.append(mean_pp)\n",
        "                        else:\n",
        "                            meanPP.append(mean_pp)\n",
        "                        count_mpp = 1\n",
        "\n",
        "                        RMSSD.append(rmssd)\n",
        "                        sdNN.append(sdnn)\n",
        "                        SDSD.append(sdsd)\n",
        "\n",
        "                        #Find non linear domain prv\n",
        "                        sd1,sd2,shan = prv_nonlinear(p,30)\n",
        "                        sd1_lst.append(sd1)\n",
        "                        sd2_lst.append(sd2)\n",
        "                        sd1_sd2.append(sd1/sd2)\n",
        "                        shanen.append(shan)\n",
        "\n",
        "                        #Find freq domain prv\n",
        "                        rri_in = interpolate(p,30)\n",
        "                        tmp1,tmp2,tmp3 = frequency_domain(rri_in, 30)\n",
        "                        hf.append(tmp1)\n",
        "                        lf.append(tmp2)\n",
        "                        lhf.append(tmp3)\n",
        "\n",
        "                        #Prepare data for predicting the drowsiness\n",
        "                        feature_data = []\n",
        "                        if len(hr_lst) >= 10:\n",
        "                            HR = add_data_s(hr_lst,data_idx)\n",
        "                            feature_data.append(HR)\n",
        "                            MPP = add_data_s(meanPP,data_idx)\n",
        "                            feature_data.append(MPP)\n",
        "                            rmssd_tmp = add_data_s(RMSSD,data_idx)\n",
        "                            feature_data.append(rmssd_tmp)\n",
        "                            SDNN = add_data_s(sdNN,data_idx)\n",
        "                            feature_data.append(SDNN)\n",
        "                            sdsd_tmp = add_data_s(SDSD,data_idx)\n",
        "                            feature_data.append(sdsd_tmp)\n",
        "                            SD2 = add_data_s(sd2_lst,data_idx)\n",
        "                            feature_data.append(SD2)\n",
        "                            SD1_SD2 = add_data_s(sd1_sd2,data_idx)\n",
        "                            feature_data.append(SD1_SD2)\n",
        "                            SHANEN = add_data_s(shanen,data_idx)\n",
        "                            feature_data.append(SHANEN)\n",
        "                            HF = add_data_s(hf,data_idx)\n",
        "                            feature_data.append(HF)\n",
        "                            LF = add_data_s(lf,data_idx)\n",
        "                            feature_data.append(LF)\n",
        "                            LHF = add_data_s(lhf,data_idx)\n",
        "                            feature_data.append(LHF)\n",
        "                            data_idx += 1\n",
        "                            feature_arr = np.array(feature_data.copy())\n",
        "                            feature_arr_re = feature_arr.reshape(-1,1)\n",
        "\n",
        "                            # Predict the result\n",
        "                            feature_transform = sc.transform(feature_arr_re.T)\n",
        "                            y_pred = NN_model.predict(feature_transform, verbose=0)\n",
        "                            result = check_predict(y_pred)\n",
        "                            print(f\"The state of the subject is: {result}\")\n",
        "                            if result == \"Alert\":\n",
        "                              count_a+=1\n",
        "                            else:\n",
        "                              count_d+=1\n",
        "                        # Update 1s data\n",
        "                        idx = idx + 30\n",
        "\n",
        "        if cv2.waitKey(20) & 0xFF == ord('q'):\n",
        "            break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "#Show the ratio between alert results / total results  and drowsy results / total results\n",
        "Alert_acc = count_a/(count_a+count_d) # for alert condition\n",
        "print(f\"Alert %: { Alert_acc*100:.2f}\")\n",
        "Drowsy_acc = count_d/(count_a+count_d) # for drowsy condition\n",
        "print(f\"Drowsy %: {Drowsy_acc*100:.2f}\")"
      ],
      "metadata": {
        "id": "UOztIIS_UvbZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}